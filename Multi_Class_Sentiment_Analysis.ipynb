{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi Class Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashsharan/Multi-Class-sentiment-analysis/blob/master/Multi_Class_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "EqydaXn2JXs7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Authentication code to read data from google drive.Run this is you are using google colab and want to acess your google drive files\n",
        "\n",
        "import os\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('drive/My Drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7kK3R2VwMhtk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, LSTM, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRz9iPzOJ81p",
        "colab_type": "code",
        "outputId": "b39dd2ef-0cf3-4a76-cbbd-d6f10940f99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "#Reading the csv files\n",
        "\n",
        "data = pd.read_csv('friends.csv')\n",
        "pd.set_option('display.max_colwidth',-1)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Scene</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{Monica} How about, youre moving!! {Rachel} Look! This is ridiculous. We should be packing you!! {Phoebe} Hey, how are you guys doing? {Rachel} Great! Monicas moving! {Monica} I am not! {Rachel} Oh really?! Then how come all your stuff is in this box?! {Phoebe} Okay, you guys. You guys I think I know whats going on here. Okay, you guys {Monica} No Phoebe I am mad! {Phoebe} Well, deep-deep-deep down! {Rachel} Yeah, Im just mad! {Phoebe} Then keep running.</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{Phoebe} Or we could just follow your clever jokes  any ideas?</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{Joey} Hey Rach! Hey, you mind if I read my comic books in here? {Rachel} Sure! Why? {Joey} Oh well, Chandler and Monica are over there and it's kinda hard to concentrate. {Rachel} What?! {Rachel} She just called and said that she was gonna be working late! {Rachel} She keeps lying to me! {Rachel} That's it! {Rachel} I'm just gonna go over there and confront them right now!</td>\n",
              "      <td>NEGATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{Joey} You forget how many great songs Heart had. {Chandler} Yeah. {Ross} You know, Barracuda was the first song I learned to play on the keyboard. {Chandler} So, you heard it, you repeated it, so that must mean you wrote it. {Joey} Oh, you guys, with this joke. I gotta say, I know I cracked up, but Im not even sure I got it. {Ross} What, you didnt get it? The doctor is a monkey. {Chandler} And monkeys cant write out prescriptions. {Chandler} You are not allowed to laugh at my joke. {Ross} Your joke? Well, I think the Hef would disagree, which is why he sent me a check for one hundred ah-dollars.</td>\n",
              "      <td>NEUTRAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{Chandler} Hey Joe! You wanna shoot some hoops? {Joey} Oh no, I cant go. Im practicing; I got an audition to be the host of a new game show.</td>\n",
              "      <td>POSTIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Scene  \\\n",
              "0  {Monica} How about, youre moving!! {Rachel} Look! This is ridiculous. We should be packing you!! {Phoebe} Hey, how are you guys doing? {Rachel} Great! Monicas moving! {Monica} I am not! {Rachel} Oh really?! Then how come all your stuff is in this box?! {Phoebe} Okay, you guys. You guys I think I know whats going on here. Okay, you guys {Monica} No Phoebe I am mad! {Phoebe} Well, deep-deep-deep down! {Rachel} Yeah, Im just mad! {Phoebe} Then keep running.                                                                                                                                                      \n",
              "1  {Phoebe} Or we could just follow your clever jokes  any ideas?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
              "2  {Joey} Hey Rach! Hey, you mind if I read my comic books in here? {Rachel} Sure! Why? {Joey} Oh well, Chandler and Monica are over there and it's kinda hard to concentrate. {Rachel} What?! {Rachel} She just called and said that she was gonna be working late! {Rachel} She keeps lying to me! {Rachel} That's it! {Rachel} I'm just gonna go over there and confront them right now!                                                                                                                                                                                                                                            \n",
              "3  {Joey} You forget how many great songs Heart had. {Chandler} Yeah. {Ross} You know, Barracuda was the first song I learned to play on the keyboard. {Chandler} So, you heard it, you repeated it, so that must mean you wrote it. {Joey} Oh, you guys, with this joke. I gotta say, I know I cracked up, but Im not even sure I got it. {Ross} What, you didnt get it? The doctor is a monkey. {Chandler} And monkeys cant write out prescriptions. {Chandler} You are not allowed to laugh at my joke. {Ross} Your joke? Well, I think the Hef would disagree, which is why he sent me a check for one hundred ah-dollars.    \n",
              "4  {Chandler} Hey Joe! You wanna shoot some hoops? {Joey} Oh no, I cant go. Im practicing; I got an audition to be the host of a new game show.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
              "\n",
              "  Sentiment  \n",
              "0  NEGATIVE  \n",
              "1  NEUTRAL   \n",
              "2  NEGATIVE  \n",
              "3  NEUTRAL   \n",
              "4  POSTIVE   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "WgFtNC3Y3zbn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(data['Scene'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cWIDCNWKsK4V",
        "colab_type": "code",
        "outputId": "62b188bf-86f1-450b-c193-7cc4fbb2c3b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "MdyGl8I3Mey-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''train, test = train_test_split(data,test_size = 0.1)\n",
        "train=train[train.Scene!=\"yolo\"]'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZBlCLT8N32z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Removing all the stopwords from the text\n",
        "\n",
        "\n",
        "scene2 = []\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_set = list(stopwords.words(\"english\"))\n",
        "for i in data['Scene']:\n",
        "  replaced = re.sub(r\"{.*?}\", '', i)\n",
        "  #reduced = filter(lambda w: w not in stopwords, re.split(r'\\W+', replaced.lower()))\n",
        "  #reduced=list(reduced)\n",
        "  #words_without_stopwords = [word for word in replaced if not word in stopwords_set]\n",
        "  #scene2.append(re.sub(r\"{.*?}\", '', i))\n",
        "  #print(replaced)\n",
        "  scene2.append(replaced)\n",
        "  \n",
        "  #print(re.sub(r\"{.*?}\", '', i))\n",
        "#print(scene2[0])\n",
        "\n",
        "scene3 = []  \n",
        "import re\n",
        "for j in scene2:\n",
        "  query = j\n",
        "  stopwords = stopwords_set\n",
        "\n",
        "  resultwords  = [word for word in re.split(\"\\W+\",query) if word.lower() not in stopwords]\n",
        "  result = ' '.join(resultwords)\n",
        "  #print(result)\n",
        "  scene3.append(result)\n",
        "print(scene3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CsNGvSBKqLME",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#word_tokens = word_tokenize(train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "53J8sZRd4wII",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "#for index, row in train.iterrows():\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6u1ElfxQMpYV",
        "colab_type": "code",
        "outputId": "dc829767-c450-4409-bc85-4bdb2ab5d8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#Toekenizing the words\n",
        "\n",
        "num_words = 500\n",
        "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                                   lower=True,split=' ')\n",
        "print(tokenizer)\n",
        "X=tokenizer.fit_on_texts(scene3)\n",
        "print(X)\n",
        "X = tokenizer.texts_to_sequences(scene3)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "max_length_of_text = 100\n",
        "X = pad_sequences(X, maxlen=max_length_of_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.preprocessing.text.Tokenizer object at 0x7f35f563d748>\n",
            "None\n",
            "Found 4943 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I4g-1PFPWB8T",
        "colab_type": "code",
        "outputId": "e679e337-20eb-4193-e074-9b2abd05d14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "X[10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  11, 161, 153,  33,  11,  34,  91, 368,\n",
              "        34,  91, 368,  14,  36,  34,  91, 368, 103, 144, 103, 202, 279,\n",
              "         5,  28,   9,  91, 186], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "1mdYgX9SMs5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y=data['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y5qOLbxZOVcc",
        "colab_type": "code",
        "outputId": "e8e1b7fe-38ab-41a4-f3b6-73d29c55a537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.19.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.8.0)\n",
            "Requirement already satisfied: pandas>=0.20.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.11.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->category_encoders) (2018.7)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.1->category_encoders) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WB82_IpeOJwy",
        "colab_type": "code",
        "outputId": "b0376f40-35bf-4de2-f919-89c383e3865b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#Onehot encoding all the 7 labels\n",
        "\n",
        "import category_encoders as ce\n",
        "le =  ce.OneHotEncoder(return_df=False, impute_missing=False, handle_unknown=\"ignore\")\n",
        "#enc= OneHotEncoder(handle_unknown='ignore')\n",
        "#enc.fit(y)\n",
        "img_label=np.array(y)\n",
        "enc=le.fit_transform(img_label)\n",
        "#image_labels_enc=img_label.reshape(-1,1)\n",
        "\"\"\"enc = OneHotEncoder()\n",
        "img_fit=enc.fit(image_labels_enc)\n",
        "img_encode=enc.transform(y).toarray()\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'enc = OneHotEncoder()\\nimg_fit=enc.fit(image_labels_enc)\\nimg_encode=enc.transform(y).toarray()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "metadata": {
        "id": "WDhzVu4YOLzb",
        "colab_type": "code",
        "outputId": "91d619d3-8acf-4697-ad09-e6c43696990d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "enc[4]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "metadata": {
        "id": "4srthqnyQ0jd",
        "colab_type": "code",
        "outputId": "cd19d184-1775-4998-dcde-7afc8de1d89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X_train, Xvalid, y_train, yvalid = train_test_split(X,enc, test_size = 0.2, random_state = 42)\n",
        "print(X_train.shape,y_train.shape)\n",
        "#print(X_test.shape,y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(800, 100) (800, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ETsSCUk5RHse",
        "colab_type": "code",
        "outputId": "9c7f4b83-b597-43ab-a050-f42994ceb441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "#LSTM model\n",
        "\n",
        "embed_dim = 1024\n",
        "lstm_out = 128\n",
        "batch_size = 64\n",
        "\n",
        "inputs = Input((max_length_of_text,))\n",
        "x = Embedding(num_words, embed_dim)(inputs)\n",
        "x = LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4)(x)\n",
        "x = Dense(4,activation='sigmoid')(x)\n",
        "model = Model(inputs, x)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_13 (Embedding)     (None, 100, 1024)         512000    \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 128)               590336    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 1,102,852\n",
            "Trainable params: 1,102,852\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_HGlIM5hwL7B",
        "colab_type": "code",
        "outputId": "b959e3b6-0e8f-48c9-9b56-f40b8cd41037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "'''model = Sequential()\n",
        "model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.7))\n",
        "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "print(model.summary())'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"model = Sequential()\\nmodel.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\\nmodel.add(SpatialDropout1D(0.7))\\nmodel.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\\nmodel.add(Dense(4, activation='softmax'))\\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\\nprint(model.summary())\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "tzR1JRbS4yjT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embed_dim, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.7))\n",
        "model.add(LSTM(128, dropout=0.7, recurrent_dropout=0.7))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "print(model.summary())'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hx3Kv3GRLPu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ti0M-1rPUKxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "early_stops = EarlyStopping(patience=3, monitor='val_acc',min_delta=0.001)\n",
        "checkpointer = ModelCheckpoint(filepath='weights4.best.eda.hdf5', verbose=1, save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VVLfWLIFRNrR",
        "colab_type": "code",
        "outputId": "32430db6-fa74-48ce-aa6d-4c7e219e25db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train,validation_data=(Xvalid, yvalid) ,batch_size = 64, epochs = 10,callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            "800/800 [==============================] - 21s 27ms/step - loss: 1.3443 - categorical_accuracy: 0.3725 - val_loss: 1.2924 - val_categorical_accuracy: 0.3150\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.29244, saving model to weights4.best.eda.hdf5\n",
            "Epoch 2/10\n",
            "800/800 [==============================] - 15s 18ms/step - loss: 1.2259 - categorical_accuracy: 0.3562 - val_loss: 1.2937 - val_categorical_accuracy: 0.3250\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 1.29244\n",
            "Epoch 3/10\n",
            "800/800 [==============================] - 14s 18ms/step - loss: 1.1963 - categorical_accuracy: 0.4550 - val_loss: 1.2714 - val_categorical_accuracy: 0.4100\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.29244 to 1.27140, saving model to weights4.best.eda.hdf5\n",
            "Epoch 4/10\n",
            "800/800 [==============================] - 15s 18ms/step - loss: 1.1428 - categorical_accuracy: 0.4500 - val_loss: 1.2740 - val_categorical_accuracy: 0.4050\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.27140\n",
            "Epoch 5/10\n",
            "800/800 [==============================] - 15s 18ms/step - loss: 1.0285 - categorical_accuracy: 0.5600 - val_loss: 1.2838 - val_categorical_accuracy: 0.4250\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.27140\n",
            "Epoch 6/10\n",
            "800/800 [==============================] - 14s 18ms/step - loss: 0.8901 - categorical_accuracy: 0.6388 - val_loss: 1.4109 - val_categorical_accuracy: 0.4350\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.27140\n",
            "Epoch 7/10\n",
            "800/800 [==============================] - 14s 18ms/step - loss: 0.7806 - categorical_accuracy: 0.6837 - val_loss: 1.5242 - val_categorical_accuracy: 0.4400\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.27140\n",
            "Epoch 8/10\n",
            "800/800 [==============================] - 14s 18ms/step - loss: 0.6632 - categorical_accuracy: 0.7350 - val_loss: 1.7489 - val_categorical_accuracy: 0.4400\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.27140\n",
            "Epoch 9/10\n",
            "800/800 [==============================] - 15s 18ms/step - loss: 0.6017 - categorical_accuracy: 0.7575 - val_loss: 1.8075 - val_categorical_accuracy: 0.4150\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.27140\n",
            "Epoch 10/10\n",
            "800/800 [==============================] - 15s 18ms/step - loss: 0.5283 - categorical_accuracy: 0.8000 - val_loss: 2.0139 - val_categorical_accuracy: 0.4250\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.27140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f35e866d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "metadata": {
        "id": "3zzktuIk-RFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('nnfl4.eda.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EMvc3zdHRURh",
        "colab_type": "code",
        "outputId": "22ffb00c-da18-4cb1-cb1d-3e7f04b0f20e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "score,acc = model.evaluate(Xvalid, yvalid, batch_size = batch_size)\n",
        "print(\"Score: %.2f\" % (score))\n",
        "print(\"Validation Accuracy: %.2f\" % (acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 1s 3ms/step\n",
            "Score: 2.01\n",
            "Validation Accuracy: 0.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ik5B95LjRqTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "train_pred = model.predict(X_train)\n",
        "train_pred = np.argmax(train_pred, axis=1)\n",
        "print(train_pred)\n",
        "f1_score(y_train, train_pred, average='samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rNKxy6ZmUOsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TCeDC4Lyc4h",
        "colab_type": "code",
        "outputId": "88c23e67-6c34-4062-e7ab-28f00c823c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Loading the test data\n",
        "data2 = pd.read_csv('test22.csv')\n",
        "data2.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Scene</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>{Phoebe} Im sorry. {Phoebe Sr} What are you doing here?! {Phoebe} I-I, came to fill your ice cube trays. {Phoebe Sr} What?! {Phoebe} Umm, okay, okay, look. {Phoebe} I took this picture from your fridge. {Phoebe} Okay, because I know that this is my Father. {Phoebe} Yeah, this is Frank Buffay and you are standing right there next to him. {Phoebe} Now, look I deserve to know where I came from. {Phoebe} All right? {Phoebe} So if you can help me find my Father then you should! {Phoebe} Otherwise, youre just mean! {Phoebe} So, just tell me the truth! {Phoebe Sr} All right, the man in the picture is Chuck Magioni. {Phoebe} My Father is Chuck Magioni? {Phoebe Sr} No, no, thats just Chuck Magioni, I-I sold him a house last year! {Phoebe Sr} And Im very sorry, but I dont know where your Father is, and thats the truth. {Phoebe} Oh. {Phoebe Sr} But umm, youre right. {Phoebe Sr} I think that a person should know where they come from. {Phoebe Sr} Wh-which is why I ah,  ahh,  okay. {Phoebe Sr} Im your mother. {Phoebe} Heh? {Phoebe Sr} Y'know I wanted to tell you yesterday, but I just, I kinda felt all floopy, and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>{Rachel} Y'know what? I cannot do this with my left hand! Would you please, help me with this too? {Rachel} Okay. Lets use this brush. {Ross} Okay. This stuff? {Rachel} Yeah. {Ross} All right. {Rachel} Careful. Light. Okay, do you know how, just sweep it across the lid. Okay? Just sweep it. {Joey} Okay-Dokey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>{Rachel} Coming. {Ross} I have a bone to pick with you. {Rachel} Uh-oh. {Ross} Yes! Ben learned a little trick. {Rachel} Oh yeah? Did he pull the old {Ross} Thats right! Thats right! {Rachel} Oh that. {Ross} Yeah that! You know I hate practical jokes! Theyre mean and theyre stupid and-and I dont want my son learning them!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>{Joey} Hey uh, is it okay to come in? {Rachel} Of course! Oh Joey, this ring I…it’s beautiful I love it! {Joey} Yeah uh look Rach, there’s something I gotta tell ya. {Rachel} Hey! {Nurse} Hey! Are you ready to try nursing again? {Rachel} Yeah! Hi Emma. Hey, why do you think she won’t take my  breast? {Nurse} It’s all right honey, it takes some babies a while to get it,  but don’t worry. It’ll happen. {Joey} Yowza {Rachel} Okay sweetie, you can do it. Just open up and put it in your  mouth. {Joey} Dear Lord. {Rachel} I’m sorry honey, what were you saying? {Joey} Oh uh-uh yeah, I think that… {Rachel} Do you think my nipples are  too big for her mouth? {Rachel} She looks scared. {Rachel} Doesn’t she  look scared? {Joey} Y’know, I don’t really know her.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>{Kristen} Umm, this is great wine. {Ross} Its from FranceIn EuropeWestern Europe. Yknow umm, a few years ago I actually was backpacking across Western Europe. {Kristen} Really? {Ross} Uh-hmmWait! It gets better. Um, yeah I was in Barcelona. {Kristen} I studied for a year in Barcelona. {Ross} Anyway, umm so I was um, I was hiking {Kristen} I love hiking! {Ross} Oh thats great! I was hiking along the foothills of Mount Tibidaybo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Index  \\\n",
              "0  1       \n",
              "1  2       \n",
              "2  3       \n",
              "3  4       \n",
              "4  5       \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Scene  \n",
              "0  {Phoebe} Im sorry. {Phoebe Sr} What are you doing here?! {Phoebe} I-I, came to fill your ice cube trays. {Phoebe Sr} What?! {Phoebe} Umm, okay, okay, look. {Phoebe} I took this picture from your fridge. {Phoebe} Okay, because I know that this is my Father. {Phoebe} Yeah, this is Frank Buffay and you are standing right there next to him. {Phoebe} Now, look I deserve to know where I came from. {Phoebe} All right? {Phoebe} So if you can help me find my Father then you should! {Phoebe} Otherwise, youre just mean! {Phoebe} So, just tell me the truth! {Phoebe Sr} All right, the man in the picture is Chuck Magioni. {Phoebe} My Father is Chuck Magioni? {Phoebe Sr} No, no, thats just Chuck Magioni, I-I sold him a house last year! {Phoebe Sr} And Im very sorry, but I dont know where your Father is, and thats the truth. {Phoebe} Oh. {Phoebe Sr} But umm, youre right. {Phoebe Sr} I think that a person should know where they come from. {Phoebe Sr} Wh-which is why I ah,  ahh,  okay. {Phoebe Sr} Im your mother. {Phoebe} Heh? {Phoebe Sr} Y'know I wanted to tell you yesterday, but I just, I kinda felt all floopy, and...   \n",
              "1  {Rachel} Y'know what? I cannot do this with my left hand! Would you please, help me with this too? {Rachel} Okay. Lets use this brush. {Ross} Okay. This stuff? {Rachel} Yeah. {Ross} All right. {Rachel} Careful. Light. Okay, do you know how, just sweep it across the lid. Okay? Just sweep it. {Joey} Okay-Dokey                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
              "2  {Rachel} Coming. {Ross} I have a bone to pick with you. {Rachel} Uh-oh. {Ross} Yes! Ben learned a little trick. {Rachel} Oh yeah? Did he pull the old {Ross} Thats right! Thats right! {Rachel} Oh that. {Ross} Yeah that! You know I hate practical jokes! Theyre mean and theyre stupid and-and I dont want my son learning them!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
              "3  {Joey} Hey uh, is it okay to come in? {Rachel} Of course! Oh Joey, this ring I…it’s beautiful I love it! {Joey} Yeah uh look Rach, there’s something I gotta tell ya. {Rachel} Hey! {Nurse} Hey! Are you ready to try nursing again? {Rachel} Yeah! Hi Emma. Hey, why do you think she won’t take my  breast? {Nurse} It’s all right honey, it takes some babies a while to get it,  but don’t worry. It’ll happen. {Joey} Yowza {Rachel} Okay sweetie, you can do it. Just open up and put it in your  mouth. {Joey} Dear Lord. {Rachel} I’m sorry honey, what were you saying? {Joey} Oh uh-uh yeah, I think that… {Rachel} Do you think my nipples are  too big for her mouth? {Rachel} She looks scared. {Rachel} Doesn’t she  look scared? {Joey} Y’know, I don’t really know her.                                                                                                                                                                                                                                                                                                                                                                                   \n",
              "4  {Kristen} Umm, this is great wine. {Ross} Its from France\n",
              "In Europe\n",
              "Western Europe. Yknow umm, a few years ago I actually was backpacking across Western Europe. {Kristen} Really? {Ross} Uh-hmmWait! It gets better. Um, yeah I was in Barcelona. {Kristen} I studied for a year in Barcelona. {Ross} Anyway, umm so I was um, I was hiking {Kristen} I love hiking! {Ross} Oh thats great! I was hiking along the foothills of Mount Tibidaybo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "1215jfdGezUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Preprocessing the test data\n",
        "\n",
        "from keras.models import load_model\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('test22.csv')\n",
        "scene = data['Scene']\n",
        "\n",
        "scene2 = []\n",
        "for i in scene:\n",
        "  replaced = re.sub(r\"{.*?}\", '', i)\n",
        "  scene2.append(replaced)\n",
        "\n",
        "stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "scene3 = []\n",
        "for j in scene2:\n",
        "  query = j\n",
        "  resultwords  = [word for word in re.split(\"\\W+\",query) if word.lower() not in stopwords_list]\n",
        "  result = ' '.join(resultwords)\n",
        "  scene3.append(result)\n",
        "\n",
        "num_words = 500\n",
        "tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n",
        "tokenizer.fit_on_texts(scene3)\n",
        "X = tokenizer.texts_to_sequences(scene3)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "max_length_of_text = 100\n",
        "X = pad_sequences(X, maxlen=max_length_of_text)\n",
        "\n",
        "#Loading the model.Write your own model name here\n",
        "model = load_model('nnfl4.eda.hdf5')\n",
        "results = model.predict(X)\n",
        "\n",
        "results = np.argmax(results, axis=1)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "conv = {0: \"NEGATIVE\",\n",
        "\t   1: \"NEUTRAL\",\n",
        "\t   2: \"POSTIVE\",\n",
        "\t   3: \"MIXED\"}\n",
        "\n",
        "for i in results:\n",
        "\tpredictions.append(conv[i])\n",
        "#Converting the solution into a CSV file\n",
        "new_df = pd.DataFrame({\"Index\": [i+1 for i in range(len(predictions))], \"Sentiment\": predictions})\n",
        "new_df.to_csv(\"solution35.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xyjdDHmLs_No",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CP7nKHASt02g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}